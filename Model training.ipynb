{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51ac30ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file created successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the community data\n",
    "data = {\n",
    "    \"CommunityName\": [\n",
    "        \"BodybuildersUnited\", \"CardioGoats\", \"YogaFlow\", \"75Hard\", \"WeightLossWarriors\",\n",
    "        \"HealthyLiving\", \"CyclingBuddies\", \"StrengthSquad\", \"RunRunners\", \"FlexibilityFreaks\",\n",
    "        \"EnduranceElite\", \"LiftLegends\", \"FitFam\", \"WellnessJourney\", \"FitGoalGetters\",\n",
    "        \"CardioWarriors\", \"YogaTribe\", \"MuscleMinds\", \"StretchMasters\", \"HIITHeroes\",\n",
    "        \"ShredSquad\", \"ZenFlex\", \"CardioKings\", \"PowerPilates\", \"RunStrong\",\n",
    "        \"BarbellBros\", \"SweatSquad\", \"BalanceBoosters\", \"IronWarriors\", \"MorningMovers\",\n",
    "        \"ActiveMoms\"\n",
    "    ],\n",
    "    \"FitnessGoal\": [\n",
    "        \"Muscle Gain\", \"Weight Loss\", \"Flexibility\", \"Weight Loss\", \"Weight Loss\",\n",
    "        \"General Fitness\", \"Endurance\", \"Muscle Gain\", \"Endurance\", \"Flexibility\",\n",
    "        \"Endurance\", \"Muscle Gain\", \"General Fitness\", \"General Fitness\", \"General Fitness\",\n",
    "        \"Endurance\", \"Flexibility\", \"Muscle Gain\", \"Flexibility\", \"Weight Loss\",\n",
    "        \"Weight Loss\", \"Flexibility\", \"Endurance\", \"Flexibility\", \"Endurance\",\n",
    "        \"Muscle Gain\", \"Weight Loss\", \"General Fitness\", \"Muscle Gain\", \"General Fitness\",\n",
    "        \"General Fitness\"\n",
    "    ],\n",
    "    \"FitnessType\": [\n",
    "        \"Strength Training\", \"Cardio\", \"Yoga & Wellness\", \"Cardio\", \"Cardio\",\n",
    "        \"Yoga & Wellness\", \"Cardio\", \"Strength Training\", \"Cardio\", \"Yoga & Wellness\",\n",
    "        \"HIIT\", \"Strength Training\", \"Yoga & Wellness\", \"Yoga & Wellness\", \"Cardio\",\n",
    "        \"Cardio\", \"Yoga & Wellness\", \"Strength Training\", \"Yoga & Wellness\", \"HIIT\",\n",
    "        \"Strength Training\", \"Yoga & Wellness\", \"Cardio\", \"Yoga & Wellness\", \"Cardio\",\n",
    "        \"Strength Training\", \"Cardio\", \"Yoga & Wellness\", \"Strength Training\", \"Cardio\",\n",
    "        \"Yoga & Wellness\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save as CSV\n",
    "df.to_csv(\"fitness_communities.csv\", index=False)\n",
    "\n",
    "print(\"CSV file created successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b74b7ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 1874 examples [00:00, 13833.74 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset columns: ['title', 'score', 'id', 'url', 'comms_num', 'created', 'body', 'timestamp', 'cleaned_text', 'tokens', 'sentiment_score', 'engagement_score', 'input_text', 'target_text']\n",
      "Example row: {'title': 'Daily Simple Questions Thread - December 11, 2021', 'score': 38, 'id': 'rdwnpn', 'url': 'https://www.reddit.com/r/Fitness/comments/rdwnpn/daily_simple_questions_thread_december_11_2021/', 'comms_num': 553, 'created': 1639216816.0, 'body': 'Welcome to the /r/Fitness Daily Simple Questions Thread - Our daily thread to ask about all things fitness. Post your questions here related to your diet and nutrition or your training routine and exercises. Anyone can post a question and the community as a whole is invited and encouraged to provide an answer. \\n        \\n# As always, be sure to [read the wiki](https://thefitness.wiki) first. Like, all of it. Rule #0 still applies in this thread.\\n        \\nAlso, there\\'s a [handy search function](https://www.reddit.com/r/Fitness/search?&restrict_sr=on) to your right, and if you didn\\'t know, you can also use Google to search r/Fitness by using the limiter \"site:reddit.com/r/fitness\" after your search topic.\\n        \\nOther good resources to check first are [Exrx.net](http://www.exrx.net/index.html) for exercise-related topics and [Examine.com](https://examine.com/) for nutrition and supplement science.\\n\\nIf you are posting a routine critique request, make sure you follow [the guidelines](https://www.reddit.com/r/Fitness/wiki/rules#wiki_rule_.239) for including enough detail.\\n        \\n**(Please note: This is not a place for general small talk, chit-chat, jokes, memes, \"Dear Diary\" type comments, shitposting, or non-fitness questions. It is for fitness questions only, and only those that are serious.)**', 'timestamp': '2021-12-11 12:00:16', 'cleaned_text': 'welcome r fitness daily simple questions thread our daily thread ask things fitness post questions related diet nutrition training routine exercises anyone post question community whole invited encouraged provide answer as always sure read wiki https thefitness wiki first like rule 0 still applies thread also handy search function https www reddit com r fitness search restrict_sr right know also use google search r fitness using limiter site reddit com r fitness search topic other good resources check first exrx net http www exrx net index html exercise related topics examine com https examine com nutrition supplement science if posting routine critique request make sure follow guidelines https www reddit com r fitness wiki rules wiki_rule_ 239 including enough detail please note this place general small talk chit chat jokes memes dear diary type comments shitposting non fitness questions it fitness questions serious', 'tokens': \"['welcome', 'r', 'fitness', 'daily', 'simple', 'questions', 'thread', 'our', 'daily', 'thread', 'ask', 'things', 'fitness', 'post', 'questions', 'related', 'diet', 'nutrition', 'training', 'routine', 'exercises', 'anyone', 'post', 'question', 'community', 'whole', 'invited', 'encouraged', 'provide', 'answer', 'as', 'always', 'sure', 'read', 'wiki', 'https', 'thefitness', 'wiki', 'first', 'like', 'rule', '0', 'still', 'applies', 'thread', 'also', 'handy', 'search', 'function', 'https', 'www', 'reddit', 'com', 'r', 'fitness', 'search', 'restrict_sr', 'right', 'know', 'also', 'use', 'google', 'search', 'r', 'fitness', 'using', 'limiter', 'site', 'reddit', 'com', 'r', 'fitness', 'search', 'topic', 'other', 'good', 'resources', 'check', 'first', 'exrx', 'net', 'http', 'www', 'exrx', 'net', 'index', 'html', 'exercise', 'related', 'topics', 'examine', 'com', 'https', 'examine', 'com', 'nutrition', 'supplement', 'science', 'if', 'posting', 'routine', 'critique', 'request', 'make', 'sure', 'follow', 'guidelines', 'https', 'www', 'reddit', 'com', 'r', 'fitness', 'wiki', 'rules', 'wiki_rule_', '239', 'including', 'enough', 'detail', 'please', 'note', 'this', 'place', 'general', 'small', 'talk', 'chit', 'chat', 'jokes', 'memes', 'dear', 'diary', 'type', 'comments', 'shitposting', 'non', 'fitness', 'questions', 'it', 'fitness', 'questions', 'serious']\", 'sentiment_score': 0.9847, 'engagement_score': 1144, 'input_text': 'Summarize: welcome r fitness daily simple questions thread our daily thread ask things fitness post questions related diet nutrition training routine exercises anyone post question community whole invited encouraged provide answer as always sure read wiki https thefitness wiki first like rule 0 still applies thread also handy search function https www reddit com r fitness search restrict_sr right know also use google search r fitness using limiter site reddit com r fitness search topic other good resources check first exrx net http www exrx net index html exercise related topics examine com https examine com nutrition supplement science if posting routine critique request make sure follow guidelines https www reddit com r fitness wiki rules wiki_rule_ 239 including enough detail please note this place general small talk chit chat jokes memes dear diary type comments shitposting non fitness questions it fitness questions serious', 'target_text': 'welcome r fitness daily simple questions thread our daily thread ask things fitness post questions related diet nutrition training routine exercises anyone post question community whole invited encouraged provide answer as always sure read wiki https thefitness wiki first like rule 0 still applies thread also handy search function https www reddit com r fitness search restrict_sr right know also use google search r fitness using limiter site reddit com r fitness search topic other good resources check first exrx net http www exrx net index html exercise related topics examine com https examine com nutrition supplement science if posting routine critique request make sure follow guidelines https www reddit com r fitness wiki rules wiki_rule_ 239 including enough detail please note this place general small talk chit chat jokes memes dear diary type comments shitposting non fitness questions it fitness questions serious'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 1236.92 examples/s]\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/training_args.py:1595: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of ü§ó Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset after tokenization: ['cleaned_text', 'input_ids', 'attention_mask', 'labels']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='189' max='189' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [189/189 41:05, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>6.334600</td>\n",
       "      <td>0.304719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.330600</td>\n",
       "      <td>0.127438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.766700</td>\n",
       "      <td>0.111966</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete! Model saved to 'fine_tuned_t5_fitness'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load preprocessed dataset\n",
    "df_text = pd.read_csv(\"processed_fitness_text_data.csv\")\n",
    "\n",
    "# Ensure required columns exist\n",
    "required_columns = [\"cleaned_text\"]\n",
    "missing_columns = [col for col in required_columns if col not in df_text.columns]\n",
    "\n",
    "if missing_columns:\n",
    "    raise ValueError(f\"Missing required columns: {missing_columns}\")\n",
    "\n",
    "# Handle missing values\n",
    "df_text[\"cleaned_text\"] = df_text[\"cleaned_text\"].fillna(\"No content available\")\n",
    "\n",
    "# Prepare dataset for fine-tuning\n",
    "df_text[\"input_text\"] = \"Summarize: \" + df_text[\"cleaned_text\"]\n",
    "df_text[\"target_text\"] = df_text[\"cleaned_text\"]\n",
    "\n",
    "# Save modified dataset\n",
    "df_text.to_csv(\"processed_fitness_text_data_modified.csv\", index=False)\n",
    "\n",
    "# Load Tokenizer & Model\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n",
    "# Load dataset using Hugging Face datasets library\n",
    "dataset = load_dataset(\"csv\", data_files=\"processed_fitness_text_data_modified.csv\")[\"train\"]\n",
    "\n",
    "# Select 500 random samples from the dataset\n",
    "dataset = dataset.shuffle(seed=42).select([i for i in range(500)])\n",
    "\n",
    "# Ensure dataset contains the correct columns\n",
    "print(\"Dataset columns:\", dataset.column_names)\n",
    "print(\"Example row:\", dataset[0])\n",
    "\n",
    "# Tokenization function with input and label processing\n",
    "def tokenize_function(examples):\n",
    "    model_inputs = tokenizer(examples[\"input_text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "    # Tokenize target text (labels) and shift left by 1 for decoder\n",
    "    labels = tokenizer(text_target=examples[\"target_text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Apply tokenization (keeping only necessary columns)\n",
    "dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"title\", \"score\", \"id\", \"url\", \"comms_num\", \"created\", \"body\", \"timestamp\", \"tokens\", \"sentiment_score\", \"engagement_score\", \"input_text\", \"target_text\"])\n",
    "\n",
    "# Ensure the dataset has 'input_ids' and 'labels'\n",
    "print(\"Dataset after tokenization:\", dataset.column_names)\n",
    "\n",
    "# Define Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",  # ‚úÖ Updated\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    fp16=False,\n",
    "    no_cuda=True\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    eval_dataset=dataset\n",
    ")\n",
    "\n",
    "# Train Model\n",
    "trainer.train()\n",
    "\n",
    "# Save Fine-Tuned Model\n",
    "model.save_pretrained(\"fine_tuned_t5_fitness\")\n",
    "tokenizer.save_pretrained(\"fine_tuned_t5_fitness\")\n",
    "\n",
    "print(\"Training complete! Model saved to 'fine_tuned_t5_fitness'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5518b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 1212.35 examples/s]\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/training_args.py:1595: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of ü§ó Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='189' max='189' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [189/189 08:40, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.321000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.935600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.345500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model 1 fine-tuning complete and saved at: fitness_t5_model_v2\n"
     ]
    }
   ],
   "source": [
    "# üì¶ Imports\n",
    "import os\n",
    "import pandas as pd\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import evaluate\n",
    "import shutil\n",
    "\n",
    "# üìå Paths\n",
    "model_output_dir = \"fitness_t5_model_v2\"\n",
    "\n",
    "# üîÑ Remove old model directory if exists\n",
    "if os.path.exists(model_output_dir):\n",
    "    shutil.rmtree(model_output_dir)\n",
    "\n",
    "# üìÑ Load datasets\n",
    "df_content = pd.read_excel(\"fitness_content_generation_pairs2.xlsx\")\n",
    "df_profile = pd.read_excel(\"processed_fitness_data.xlsx\")\n",
    "\n",
    "# üìù Create 'community_type' by combining 'Fitness Goal' and 'Fitness Type'\n",
    "df_profile['community_type'] = df_profile['Fitness Goal'] + \" | \" + df_profile['Fitness Type']\n",
    "\n",
    "# üìä Assign random community type to each content sample (since no direct mapping)\n",
    "df_content['community_type'] = df_profile['community_type'].sample(n=len(df_content), replace=True, random_state=42).values\n",
    "\n",
    "# üîç Sample 500 records for quick fine-tuning (or adjust if needed)\n",
    "df_sampled_content = df_content.sample(n=500, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# üìö Convert to Hugging Face Dataset\n",
    "dataset = Dataset.from_pandas(df_sampled_content)\n",
    "\n",
    "# üî† Load tokenizer and model\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n",
    "# üìä Load ROUGE evaluation metric\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "# ‚úÇÔ∏è Preprocessing function: tokenize inputs and targets\n",
    "def preprocess_function(examples):\n",
    "    inputs = tokenizer(\n",
    "        [f\"Community: {community} | {text}\" for community, text in zip(examples['community_type'], examples['input_text'])],\n",
    "        truncation=True, padding=\"max_length\", max_length=256\n",
    "    )\n",
    "    targets = tokenizer(\n",
    "        examples['target_text'],\n",
    "        truncation=True, padding=\"max_length\", max_length=128\n",
    "    )\n",
    "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "# üîÑ Apply preprocessing\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# üìä Compute ROUGE metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    return result\n",
    "\n",
    "# ‚öôÔ∏è Training configuration\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_output_dir,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=5e-5,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    fp16=False,\n",
    "    no_cuda=True,  # CPU mode for macOS M3\n",
    "    overwrite_output_dir=True,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# üèãÔ∏è Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets,\n",
    "    eval_dataset=tokenized_datasets,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# üöÄ Train the model\n",
    "trainer.train()\n",
    "\n",
    "# üíæ Save model and tokenizer\n",
    "model.save_pretrained(model_output_dir)\n",
    "tokenizer.save_pretrained(model_output_dir)\n",
    "\n",
    "print(\"‚úÖ Model 1 fine-tuning complete and saved at:\", model_output_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
